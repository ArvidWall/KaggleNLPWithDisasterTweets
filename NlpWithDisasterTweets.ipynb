{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful reading:  \n",
    "The illustrated transformer: http://jalammar.github.io/illustrated-transformer/  \n",
    "Attention is all you need: https://arxiv.org/pdf/1706.03762.pdf  \n",
    "BERT: https://arxiv.org/pdf/1810.04805.pdf  \n",
    "A Visual Guide to Using BERT for the First Time: http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/  \n",
    "Tensorflow hub: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert import tokenization\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable = True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text_input):\n",
    "    for text in text_input:\n",
    "        input_ids = []\n",
    "        sequence_ids = []\n",
    "        attention_mask_ids = []\n",
    "        input_tokens = tokenizer.tokenize(text)\n",
    "        input_tokens = ['[CLS]'] + input_tokens + ['[SEP]']\n",
    "        padding_len = max_len - len(input_tokens)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        token_ids += [0] * padding_len\n",
    "        attention_mask = [1] * len(input_tokens) + [0] * padding_len\n",
    "        sequence = [1] * max_len\n",
    "\n",
    "        attention_mask_ids.append(attention_mask)\n",
    "        sequence_ids.append(sequence)\n",
    "        input_ids.append(token_ids)\n",
    "    \n",
    "        sequence_ids = np.array(sequence_ids)\n",
    "        input_ids = np.array(input_ids)\n",
    "        attention_mask_ids = np.array(attention_mask_ids)\n",
    "        return input_ids, attention_mask_ids, sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "labels = np.array(train_df.target)\n",
    "train_input = encode_text(train_df.text)\n",
    "test_input = encode_text(test_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 768)]        0           keras_layer_1[2][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            769         tf_op_layer_strided_slice_2[0][0]\n",
      "==================================================================================================\n",
      "Total params: 109,483,010\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 512\n",
    "input_tokens_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "attention_mask_ids= Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")  \n",
    "sequence_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "inputs = [input_tokens_ids, attention_mask_ids, sequence_ids]\n",
    "pooled_output, sequence_output = bert_layer(inputs)\n",
    "outputs = Dense(1, activation = 'sigmoid')(sequence_output[:, 0, :])\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.01)\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy', metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file_path = \"model_checkpoint_weights/ weights.{epoch:02d}-{val_loss:.2f}.h5\"\n",
    "model_checkpoint = ModelCheckpoint(filepath = checkpoint_file_path, \n",
    "                                   save_weights_only = True,\n",
    "                                   monitor = 'val_acc', \n",
    "                                   mode = 'max',\n",
    "                                   save_best_only = True,\n",
    "                                   verbose = True)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\",\n",
    "                                min_delta=0.01,\n",
    "                                patience=3,\n",
    "                                verbose=1,\n",
    "                                mode=\"auto\",\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7613 samples\n"
     ]
    }
   ],
   "source": [
    "model.fit(input_data, labels, epochs = 10, batch_size = 32, callbacks = [model_checkpoint, early_stoppping], validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = \"\"\n",
    "model.load_weights(weight_file)\n",
    "pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-645668f8e010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkaggle_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkaggle_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "kaggle_submission = pd.DataFrame(columns = ['target'])\n",
    "kaggle_submission['target'] = pred.round().astype(int)\n",
    "kaggle_submission.to_csv('kaggle_submission.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
